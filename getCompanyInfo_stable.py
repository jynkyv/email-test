import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import threading
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_URL = "https://www.gai-rou.com"
LIST_PAGE_URL = BASE_URL + "/kanri_list/"

# åˆ›å»ºæ›´ç¨³å®šçš„session
session = requests.Session()
retry_strategy = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504],
)
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("http://", adapter)
session.mount("https://", adapter)

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
}

# çº¿ç¨‹å®‰å…¨çš„é”å’Œå…±äº«æ•°æ®
data_lock = Lock()
all_companies = []
processed_count = 0
total_count = 0

def get_company_links(page=1):
    """è·å–åˆ—è¡¨é¡µä¸Šçš„æ‰€æœ‰å…¬å¸å†…é¡µé“¾æ¥"""
    url = LIST_PAGE_URL + f"page/{page}/"
    
    for attempt in range(3):  # é‡è¯•3æ¬¡
        try:
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ... (å°è¯• {attempt + 1}/3)")
            
            # ä½¿ç”¨sessionè€Œä¸æ˜¯requests.get
            res = session.get(url, headers=headers, timeout=30, verify=False)
            
            # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
            if res.status_code != 200:
                print(f"âŒ é¡µé¢ {page} åŠ è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {res.status_code}")
                if attempt < 2:
                    time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
                    continue
                return []

            soup = BeautifulSoup(res.text, "html.parser")
            
            links = []
            # é¦–å…ˆæ‰¾åˆ°æ‰€æœ‰ box_dantai_list å®¹å™¨
            box_containers = soup.find_all("div", class_="box_dantai_list")
            
            # åœ¨æ¯ä¸ª box_dantai_list å®¹å™¨ä¸­æå–é“¾æ¥
            for i, box in enumerate(box_containers):
                all_links_in_box = box.find_all("a", href=True)
                
                for j, a_tag in enumerate(all_links_in_box):
                    href = a_tag["href"]
                    
                    # æ£€æŸ¥é“¾æ¥æ˜¯å¦åŒ…å« "/kanri/" å¹¶ä¸”æ˜¯æœ‰æ•ˆçš„ä¼ä¸šé¡µé¢é“¾æ¥
                    if "/kanri/" in href:
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        if href.startswith("/"):
                            full_url = BASE_URL + href
                        # å¤„ç†ç»å¯¹è·¯å¾„
                        elif href.startswith("http"):
                            full_url = href
                        # å¤„ç†å…¶ä»–ç›¸å¯¹è·¯å¾„
                        else:
                            full_url = BASE_URL + "/" + href
                        
                        links.append(full_url)

            print(f"ğŸ“„ ç¬¬ {page} é¡µæ‰¾åˆ° {len(box_containers)} ä¸ªå®¹å™¨ï¼ŒæŠ“å–åˆ° {len(links)} ä¸ªä¼ä¸šé“¾æ¥")
            return list(set(links))  # set è‡ªåŠ¨å»é‡
            
        except Exception as e:
            print(f"âŒ æŠ“å–ç¬¬ {page} é¡µå¤±è´¥ (å°è¯• {attempt + 1}/3): {e}")
            if attempt < 2:
                time.sleep(10)  # ç­‰å¾…10ç§’åé‡è¯•
            else:
                return []
    
    return []

def get_company_info(company_url):
    """ä»å…¬å¸å†…é¡µä¸­æå–ç‰¹å®šä¿¡æ¯"""
    for attempt in range(3):  # é‡è¯•3æ¬¡
        try:
            # ä½¿ç”¨sessionè€Œä¸æ˜¯requests.get
            res = session.get(company_url, headers=headers, timeout=30, verify=False)
            soup = BeautifulSoup(res.text, "html.parser")
            
            data = {'URL': company_url}
            
            # æŸ¥æ‰¾æ‰€æœ‰ dl_dantai_list å…ƒç´ 
            dl_elements = soup.find_all("dl", class_="dl_dantai_list")
            
            print(f"ğŸ” æ‰¾åˆ° {len(dl_elements)} ä¸ª dl_dantai_list å…ƒç´ ")
            
            # æ‰¾åˆ°å½“å‰ä¼ä¸šçš„ä¸»è¦ä¿¡æ¯åŒºåŸŸ
            current_company_data = {}
            
            for i, dl in enumerate(dl_elements):
                dt = dl.find("dt")
                dd = dl.find("dd")
                
                if dt and dd:
                    label = dt.text.strip()
                    
                    # å¯¹äºåœ°å€å­—æ®µï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ä»¥æ’é™¤Google Mapsé“¾æ¥
                    if "ä½æ‰€" in label:
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«Google Mapsé“¾æ¥
                        google_maps_link = dd.find("a", href=lambda x: x and "maps.google.com" in x)
                        if google_maps_link:
                            # è·å–é“¾æ¥å‰çš„æ–‡æœ¬å†…å®¹ï¼ˆåœ°å€ï¼‰
                            text_parts = []
                            for content in dd.contents:
                                if isinstance(content, str):
                                    text_parts.append(content.strip())
                            value = ' '.join(text_parts).strip()
                            print(f"ğŸ“ ç¬¬{i+1}ä¸ªå…ƒç´  - åœ°å€(å¸¦Google Maps): {value}")
                        else:
                            # å¦‚æœæ²¡æœ‰Google Mapsé“¾æ¥ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•
                            text_parts = []
                            for content in dd.contents:
                                if isinstance(content, str):
                                    text_parts.append(content.strip())
                            value = ' '.join(text_parts).strip()
                            print(f"ğŸ“ ç¬¬{i+1}ä¸ªå…ƒç´  - åœ°å€(æ— Google Maps): {value}")
                    else:
                        value = dd.text.strip()
                    
                    # åªä¿å­˜ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ¯ä¸ªå­—æ®µï¼ˆé€šå¸¸æ˜¯å½“å‰ä¼ä¸šçš„ä¿¡æ¯ï¼‰
                    if "å›£ä½“å" in label and "å›£ä½“å" not in current_company_data:
                        current_company_data['å›£ä½“å'] = value
                        print(f"ğŸ¢ ç¬¬{i+1}ä¸ªå…ƒç´  - å›£ä½“å: {value}")
                    elif "ä½æ‰€" in label and "ä½æ‰€" not in current_company_data:
                        # åªä¿å­˜å¸¦æœ‰Google Mapsé“¾æ¥çš„åœ°å€
                        google_maps_link = dd.find("a", href=lambda x: x and "maps.google.com" in x)
                        if google_maps_link:
                            current_company_data['ä½æ‰€'] = value
                            print(f"âœ… ä¿å­˜å¸¦Google Mapsçš„åœ°å€: {value}")
                        else:
                            print(f"âš ï¸ è·³è¿‡æ— Google Mapsçš„åœ°å€: {value}")
                    elif "FAXç•ªå·" in label and "FAXç•ªå·" not in current_company_data:
                        current_company_data['FAXç•ªå·'] = value
                        print(f"ğŸ“ ç¬¬{i+1}ä¸ªå…ƒç´  - FAX: {value}")
                    elif "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹" in label and "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹" not in current_company_data:
                        current_company_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] = value
                        print(f"ğŸ“§ ç¬¬{i+1}ä¸ªå…ƒç´  - ãƒ¡ãƒ¼ãƒ«: {value}")
            
            # å°†æ‰¾åˆ°çš„æ•°æ®åˆå¹¶åˆ°dataä¸­
            data.update(current_company_data)
            
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ä¸ºç©ºå­—ç¬¦ä¸²
            required_fields = ['å›£ä½“å', 'ä½æ‰€', 'FAXç•ªå·', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹']
            for field in required_fields:
                if field not in data:
                    data[field] = ""
            
            return data
            
        except Exception as e:
            print(f"âŒ æŠ“å–ä¼ä¸šä¿¡æ¯å¤±è´¥ (å°è¯• {attempt + 1}/3): {company_url}, é”™è¯¯: {e}")
            if attempt < 2:
                time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
            else:
                return None
    
    return None

def process_company(company_url):
    """å¤„ç†å•ä¸ªä¼ä¸šä¿¡æ¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global processed_count
    
    info = get_company_info(company_url)
    if info:
        with data_lock:
            all_companies.append(info)
            processed_count += 1
            print(f"âœ… [{processed_count}/{total_count}] {info.get('å›£ä½“å', 'æœªçŸ¥ä¼ä¸š')} - FAX: {info.get('FAXç•ªå·', 'N/A')}")
            print(f"ğŸ“ åœ°å€: {info.get('ä½æ‰€', 'N/A')}")  # æ˜¾ç¤ºå½“å‰ä¼ä¸šçš„åœ°å€
    
    return info

def process_page(page_num):
    """å¤„ç†å•ä¸ªé¡µé¢çš„æ‰€æœ‰ä¼ä¸šï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    print(f"ğŸ”„ å¼€å§‹å¤„ç†ç¬¬ {page_num} é¡µ...")
    
    # è·å–è¯¥é¡µé¢çš„æ‰€æœ‰ä¼ä¸šé“¾æ¥
    page_links = get_company_links(page_num)
    
    if not page_links:
        print(f"âŒ ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°ä¼ä¸šé“¾æ¥")
        return []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†è¯¥é¡µé¢çš„æ‰€æœ‰ä¼ä¸šï¼Œå‡å°‘å¹¶å‘æ•°
    with ThreadPoolExecutor(max_workers=3) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_url = {executor.submit(process_company, url): url for url in page_links}
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result()
                time.sleep(2)  # æ¯ä¸ªä¼ä¸šå¤„ç†åç­‰å¾…2ç§’
            except Exception as e:
                print(f"âŒ å¤„ç†ä¼ä¸šå¤±è´¥: {url}, é”™è¯¯: {e}")
    
    print(f"âœ… ç¬¬ {page_num} é¡µå¤„ç†å®Œæˆï¼Œæ‰¾åˆ° {len(page_links)} ä¸ªä¼ä¸š")
    return page_links

def save_progress(start_page, end_page):
    """ä¿å­˜å½“å‰è¿›åº¦"""
    global all_companies
    
    if all_companies:
        df = pd.DataFrame(all_companies)
        columns_order = ['å›£ä½“å', 'ä½æ‰€', 'FAXç•ªå·', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'URL']
        df = df[columns_order]
        
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = f"gai-rou_companies_pages_{start_page}-{end_page}_{timestamp}.csv"
        
        df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
        print(f"âœ… å·²ä¿å­˜ {len(all_companies)} ä¸ªä¼ä¸šæ•°æ®åˆ°: {csv_filename}")
        return csv_filename
    return None

# ä¸»ç¨‹åº
if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æŠ“å–æŒ‡å®šé¡µé¢èŒƒå›´çš„ä¼ä¸šä¿¡æ¯')
    parser.add_argument('--start', type=int, default=1, help='èµ·å§‹é¡µç  (é»˜è®¤: 1)')
    parser.add_argument('--end', type=int, default=185, help='ç»“æŸé¡µç  (é»˜è®¤: 185)')
    parser.add_argument('--workers', type=int, default=2, help='å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 2)')
    
    args = parser.parse_args()
    
    start_page = args.start
    end_page = args.end
    max_workers = args.workers
    
    print(f"ğŸš€ å¼€å§‹æŠ“å–é¡µé¢ {start_page} åˆ° {end_page} çš„ä¼ä¸šä¿¡æ¯...")
    print(f"ğŸ“Š æ€»å…±éœ€è¦å¤„ç† {end_page - start_page + 1} é¡µ")
    print(f"âš™ï¸ å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
    print(f"ğŸ”’ SSLéªŒè¯: å·²ç¦ç”¨")
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†é¡µé¢ï¼Œå‡å°‘å¹¶å‘æ•°
    with ThreadPoolExecutor(max_workers=max_workers) as page_executor:
        # æäº¤æ‰€æœ‰é¡µé¢ä»»åŠ¡
        future_to_page = {page_executor.submit(process_page, page): page for page in range(start_page, end_page + 1)}
        
        # ç­‰å¾…æ‰€æœ‰é¡µé¢å¤„ç†å®Œæˆ
        for future in as_completed(future_to_page):
            page_num = future_to_page[future]
            try:
                page_links = future.result()
                total_count += len(page_links)
                time.sleep(5)  # æ¯é¡µå¤„ç†åç­‰å¾…5ç§’
            except Exception as e:
                print(f"âŒ å¤„ç†ç¬¬ {page_num} é¡µå¤±è´¥: {e}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    print(f"\nğŸ‰ é¡µé¢ {start_page}-{end_page} å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {processed_count} ä¸ªä¼ä¸š")
    
    csv_filename = save_progress(start_page, end_page)
    
    if csv_filename:
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {os.path.getsize(csv_filename) / 1024:.1f} KB")
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        df = pd.DataFrame(all_companies)
        columns_order = ['å›£ä½“å', 'ä½æ‰€', 'FAXç•ªå·', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'URL']
        df = df[columns_order]
        print(f"\nğŸ“‹ æ•°æ®é¢„è§ˆï¼š")
        print(df.head())
    else:
        print("âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•ä¼ä¸šä¿¡æ¯") 